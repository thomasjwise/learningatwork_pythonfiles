{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Wrangling in Python\n",
    "### Thomas Wise\n",
    "### May 17th 2022 \n",
    "\n",
    "## Introduction \n",
    "\n",
    "Welcome! This document is written to accompany *Introduction to Data Wrangling in Python*, providing further elaboration of the topics covered in the accompanying presentation. This also acts as a walk through for the practical exercises completed during the session, with worksheets also available in the provided zip files. \n",
    "\n",
    "### Session Objectives \n",
    "\n",
    "- To be introduced to useful Python data manipulation packages\n",
    "- To understand the importance of data manipulation and cleaning for good analytic practices \n",
    "- To understand how to clean, manipulate, select and transform data both in standardized and conditional situations. \n",
    "\n",
    "### Transferable Skills \n",
    "\n",
    "- Data Analytics\n",
    "- Presenting and Disseminating Data Effectively \n",
    "- Basic Introduction to the use of Python\n",
    "- Basic Data Manipulation Techniques \n",
    "- Basic Data Cleaning Theory and Techniques\n",
    "- Creative problem solving & debugging\n",
    "\n",
    "### Schedule \n",
    "\n",
    "- Part 1: Understanding Data\n",
    "    - Section 1: Viewing & Summarizing Data \n",
    "    - Section 2: Data Types and converting between them \n",
    "    - Section 3: Missing Data \n",
    "- Part 2: Handling Data \n",
    "    - Section 1: Selecting Variables \n",
    "    - Section 2: Transforming and Mutating Data\n",
    "    - Section 3: Bringing everything together\n",
    "\n",
    "    \n",
    "### Software, Files and Content \n",
    "\n",
    "This workshop, files and content is available in the accompanying .zip file. If you do not have Anaconda Navigator installed on your machine, you can access an interactive form of this document (so you can complete the exercises) through this [Binder Link](<https://notebooks.gesis.org/binder/jupyter/user/thomasjwise-lea-ork_pythonfiles-i7z7qmxw/lab>). \n",
    "\n",
    "Throughout this session, we will be using the **[Midwest Demographics Data Set](<https://ggplot2.tidyverse.org/reference/midwest.html>)**. This is a data set originally available from the [tidyverse](<https://www.tidyverse.org/>) for R, however is also accessible online. This data set contains demographic information from the 2000 census in the USA, relating to the Midwest States, these 437 observations span 28 variables including county, population information and other demographic information. This will form a foundation for many of our sessions as this provides data easily suited to our initial investigations. \n",
    "\n",
    "To access this data, you can run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the required packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "# Load the packages\n",
    "midwest_data = pd.read_csv(\"http://goo.gl/G1K41K\") \n",
    "\n",
    "#print(midwest_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Part 1: Understanding Data**\n",
    "\n",
    "It is said that approximately 80-90% of a statisticians, data scientists or analysts time is spent cleaning, manipulating or exploring presented data. During as such, this handout and accompanying interactive exercises will dive into both the reasons for these processes, in addition to how to undertake basic cleaning, manipulation, selection and transformation of data, during both standardized and conditional situations, whilst upholding good programming practice. \n",
    "\n",
    "The topic of data is complex, with data not only increasing in scale and complexity as data becomes more accessible, but also in prevalence in every day life. With both individual and business level decisions being influenced through data, whether deciding the pay rise of employees in line with national performance, or simply the route home due to live and predicted traffic information. \n",
    "\n",
    "Throughout this session, we will explore the some of the first steps important to making these decisions, that is how to efficiently wrangle, handle and clean data ready for further exploration, modeling or other purpose. With the intention of this session to help prepare you for the hackathon later this week, as these data sets will undoubtedly require some form of manipulation to be effectively visualized. \n",
    "\n",
    "### 1.1: Viewing & Summarizing Data \n",
    "\n",
    "Viewing and Summarizing data is one of the most important steps, as this can help you understand the data in more depth. Although you can certainly use many different visualization techniques which we have already discussed, you can also use other statistical methods as well as functions which display only a sample of the provided data. \n",
    "\n",
    "When considering viewing data, you will want to consider the following functions: \n",
    "\n",
    "- `print()` - Print the data \n",
    "- `.shape()` - Data Frame rows and columns \n",
    "- `.head() / .tail()` - Shows the first or last 5 rows\n",
    "- `.dtypes` - Shows the types of the columns\n",
    "\n",
    "\n",
    "#### Exercise 1a: Using one of the techniques mentioned, or one which you know of, view the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although viewing your data set can be useful in initially understanding key components of your data like the shape, structure, and type. Summarizing your data through descriptive statistics functions (`np.mean()`,  `statistics.stdev()`) or the generating the overall summary, through the `df.describe()` function. Can help you get a better idea of the individual variables present within the data set. This includes information such as frequencies of missingness and categorical variables in addition to the previously mentioned descriptive statistics. \n",
    "\n",
    "#### Exercise 1b: Use the `df.describe()` function to generate a summary of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Data Types and Converting between them \n",
    "\n",
    "Understanding the types of data present in your data set is incredibly important. As the type of data present can impact not only the methods which can be used to effectively visualize the data, but also how it can and should be handled, cleaned and transformed. A good example of this is poorly labelled categorical data. In the midwest data set which we are using today; state could be considered a categorical variable, however, as it is expressed as a string data type (for example \"IL\") this means it is not currently possible to explore the distribution of different states within the data. However, before we can learn to correct the data type, we should consider what data types are available: \n",
    "\n",
    "- **Integers** - whole numbers with no decimal places\n",
    "- **Floating-point Numbers** - real or with decimals\n",
    "- **Complex Numbers** - complex numbers including imaginary numbers \n",
    "- **Strings** - words or strings of letters \n",
    "- **Boolean Types** - for example TRUE or FALSE\n",
    "- **category** - values which take on a factor or one of selected values\n",
    "\n",
    "It should be noted also there are multiple other complex data types, including those related to time and dates, however due to limitations in today's sessions we will not cover this. If you are interested however in how to handle time data, please refer to [Python Data Science Handbook](<https://jakevdp.github.io/PythonDataScienceHandbook/>) by Jake VanderPlas or [Real Pythons Guide to Python Time Module](<https://realpython.com/python-time-module/>) by Alex Ronquillo. \n",
    "\n",
    "When considering the conversion of data between types, it is important not only to ensure you are converting into a semantically relevant type (numeric values as characters do not make much sense personally), but also that you are not loosing information through the conversion. For example, take numeric variables compared to integers, numeric values have the potential for an (almost) infinite number of decimal places, whereas integers have none, and therefore round accordingly. This in some situations is perfectly acceptable for situations such as total population, where it can only be a whole number, however for options such as percentages, density or size decimal places are required to demonstrate this precision. The problem however is that once converted from numeric to integer, it is not possible to regain this lost information, so tread carefully! \n",
    "\n",
    "Below you will find a list once again of data types and their most common conversion function: \n",
    "\n",
    "- **Integers** - `int()`\n",
    "- **Floating-point Numbers** - `float()` \n",
    "- **Complex Numbers** - `complex()`\n",
    "- **Strings** - `str()`\n",
    "- **Boolean Types** - `bool()`\n",
    "- **category** - `.astype('category')`\n",
    "\n",
    "#### Exercise 2a: \n",
    "Convert the state variables in the data from character to factor using the `.astype(category)` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2b: \n",
    "To better understand the influence of rounding and variable types, convert the area variable from numeric to integer, before converting it back. Using summary before and after, see how this has influenced the variable.\n",
    "\n",
    "Note: Once you have completed exercise 2b, ensure to rerun the data loading code to prevent influence in later exercises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Missing Data\n",
    "\n",
    "Although the data set selected does not include missing data, it is still an important topic to discuss, even if only in theory. In the real world, missing data can occur for a host of reasons. However, they are widely considered to be grouped under three categories, **Missing Completely at Random (MCAR)**, **Missing at Random (MAR)** and **Missing not at Random (MNAR)**. \n",
    "\n",
    "- **MCAR** is considered when the missing data has no reasonable link to any specific value or variables. That is to say that the missingness has resulted purely at random, and therefore out of the control of the participants themselves. As such, MCAR missingness could result from study design, equipment failure or simple technical errors. \n",
    "- **MAR** is considered when the missing data has a potential to be the result of a set of observed responses, which are though not related to the specific missing values which are present. \n",
    "- **MNAR** is considered when data is missing for another reason, or a reason which can be explained, beyond pure randomness. For example drop out in a medical study due to recovery or death, but this not being accounted for. This is highly problematic for complex models as this can result in significant bias. \n",
    "\n",
    "However, when considering how to identify missing data, Python presents several options: \n",
    "\n",
    "- For missing values per variable (columns) \n",
    "    - `.isna().sum(axis = 0)` \n",
    "    - `.isnull().sum()`\n",
    "    - \n",
    "\n",
    "- For missing values per observation (rows)\n",
    "    - `.isna().sum(axis = 1)\n",
    "\n",
    "For this section, let us look at missingness in relation to the [planets data set](<https://exoplanets.nasa.gov/discovery/exoplanet-catalog/>). This data set from NASA, explores exoplanets beyond our solar system. This data set can be extracted from the seaborn package, and can be done so through the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "planets_data = sns.load_dataset('planets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3a: \n",
    "Using one of the methods mentioned above, identify which variables have missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3b: \n",
    "View the data set as a whole and select one planet which has at least one missing observation or data point. Once selected, calculate how many other planets share that missing value and how many share that number of missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we consider missing data, one question frequently arises regarding how do we handle it. And typically we are met with three responses: **Remove** the missing points, **Replace** the missing data with something else or **Retain** the missing data within the data. Each technique has advantages and disadvantages, although we are unable to dive into each of the specific techniques here today as it is a major debate within the sciences, below you will find a mini description of each. \n",
    "\n",
    "- **Retain**: Missing data is retained within the data set, this like other data still holds value, since it could be missing due to a specific reason not yet considered in the model, or could be part of a specific group which if removed would not be represented. \n",
    "- **Remove**: Arguably one of the simplest methods, observations or variables with a high percentage of missing values are removed and discarded. This although useful as it removes potential gaps, this can be an issue for sparse data with large amounts of missingness, resulting in a large reduction in observations. \n",
    "- **Replace**: One of the more complex and debated methods, since this is where missing data points are filled in with information which describes a certain condition. For example, all missing data is replaced by the average of all other data points; or they can be replaced as a result of a regression or imputation equation which completes data. \n",
    "\n",
    "#### Exercise 3c: \n",
    "To examine the potential negative impact of removing data, remove all observations which have missing values\n",
    "\n",
    "Hint: you can use the function `df.dropna(axis = 0, how = 'any', inplace = True)`\n",
    "\n",
    "Once completed, consider how many observations have been removed, and the impact this may have upon the quality of the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Part 2: Handling Data**\n",
    "\n",
    "For the second half of this session, we will be focusing upon the fundamentals of data manipulation, both under standard static conditions, but also conditionally. Data manipulation can be important for multiple reasons, not only as part of the cleaning process, but also to make data sets more manageable and to create new variables which are a better representation of the data whether through grouping or transformation. \n",
    "\n",
    "### 2.1: Selecting Variables\n",
    "\n",
    "When using Python, there are multiple different ways in which you can either select or remove, variables or observations. You may want to select a specific column, observation or group of observations, either in specifically or as a result of conditional requirements. If you want to select variables you have many different methods at your disposal. \n",
    "\n",
    "- Selecting Variable (Columns) \n",
    "    - Name the column specifically: *midwest_data[\"state\"]* \n",
    "    - Name multiple columns: *midwest_data[[\"state\", \"county\"]]\n",
    "    - Slice the data using `.iloc[]`: *midwest_data.iloc[:,0:2]*\n",
    "    - Slice the data using `.loc[]`: *midwest_data.loc[:,'state':'percollege']*\n",
    "    - Filter using `.filter()`: *midwest_data.filter(['state','percollege'])*\n",
    "- Selecting Observations (Rows)\n",
    "    - Slice the data using `iloc[]`: *midwest_data.iloc[0:10,:]*\n",
    "    - Slice the data using `loc[]`: *midwest_data.loc[[570:590],'PID':'percollege']*\n",
    "    - Select Conditionally: \n",
    "        - Using `.loc()`: *midwest_data.loc[midwest_data['state'] == 'IL', 'state':'percollege']*\n",
    "        - Using string requests:\n",
    "            - `str.endswith()`\n",
    "            - `str.startswith()`\n",
    "            - `isin()`\n",
    "            \n",
    "When using conditional statements, it is important to ensure your selecting operators correct: \n",
    "\n",
    "- Operators: \n",
    "  - `==` is exactly equal too \n",
    "  - `>` is greater than\n",
    "  - `<` is less than \n",
    "  - `>=` is greater than or equal to\n",
    "  - `<=` is less than or equal to\n",
    "  - `!=` is not equal to\n",
    "  - `&` union (and)\n",
    "  - `|` or \n",
    "\n",
    "#### **Practical Exercise 4a:**\n",
    "Using the midwest_data once again, create a new data set called midwest_data_2, which contains only the variables: state, area, poptotal, popdensity, popadults, perchsd, percollege, percprof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Practical Exercise 4b:**\n",
    "Using the data set created in 4a, create the 3 individual data sets which follow these conditions:\n",
    "\n",
    "- Contains only data from the state IL\n",
    "- Contains only data from the state IL and those with an area over 0.05\n",
    "- Contains Adult Populations over 50,000, with a perchsd over 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Transforming and Mutating Data\n",
    "\n",
    "To improve the utility or accessibility of data, variables can be transformed using any number of mathematical functions. This includes the both the addition of external information, transformation to follow a psychometric guide or the interaction with another variable within the data (for example summation). \n",
    "\n",
    "Assuming you intend to create a new column within a data frame, based upon other variables, two common methods occur, **Element-Wise Operation** or **apply()** function. \n",
    "\n",
    "The **Element-Wise Operation** uses core python principles in order to create the new column. \n",
    "\n",
    "`data_frame['New_column'] = data_frame['ColA'] + data_frame['ColB'] `\n",
    "\n",
    "In this case, we will see that the 'New Column' is simply formed as the summation of Columns A and B of the data frame. Similarity this also occurs with the apply() function. \n",
    "\n",
    "`data_frame['New_column'] = data_frame.apply(lambda row: row.A + row.B), axis = 1)` \n",
    "\n",
    "This will result in the same output. However, this transformation and mutation is not limited to summation of variables, as it can actively apply *almost* any mathematical function available in Python. \n",
    "\n",
    "#### Exercise 5a: \n",
    "Create new variables, indicating the estimated population of each observation for those below the poverty line\n",
    "\n",
    "*Hint: For this, use the variables percbelowpoverty, percchildbelowpovert, percadultpoverty and percelderlypoverty and poptotal to calculate these new variables.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Bringing everything together\n",
    "#### Exercise 6a:\n",
    "Using all the skills learnt from today's session, attempt to complete the tasks below: \n",
    "\n",
    "- Generate a limited data set containing (only the following variables)\n",
    "  - All variables relating to the percentage of ethnic (percwhite) makeup of each state, in addition to the population total and state itself. \n",
    "\n",
    "- Generate a limited data set containing: \n",
    "  - Observations from the state \"MI\" or \"IN\"\n",
    "  - With a population over 50,000 and an area over 0.01\n",
    "  \n",
    "- Generate a limited data set containing: \n",
    "  - Observations from the states \"MI\", \"IL\" or \"OH\"\n",
    "  - Create 3 new columns using mutate, to estimate the number of people at each level of education (perchsd, percollege, percprof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
